UdaConnect application consists of 3 parts: frontend: udaconnect-app, backend: udaconnect-api, database: postgres. 

I decided to refactor the backend "udaconnect-api" monolith app by separating it to 3 entity microservices: Persons, Locations, Connections.
1) "Persons" entity-> could be separated into its own microservice container "persons-api", it has the only dependancy on the database. It exposes 2 API endpoints:
http://localhost:30001/api/persons
http://localhost:30001/api/persons/<person_id>
The complexity of refactoring: lowest, the most simple part.

2) "Connections" entity-> could be separated into its own microservice container. Initial design of that entity was set to fetch persons and locations data from the database by making the map of where the specified by frontend app person was located, making that requests by the response to frontend app to its API endpoint:
http://localhost:30002/api/persons/<person_id>/connection
Both requests to fetch persons and locations data were used to send to the postgres database. I decided to rearrange the request dedicated to persons by routing it to the "http://localhost:30001/api/persons" of "persons-api" microservice. So, I also thought about the similar approach regarding the locations data, but the overall complexity of the Locations entity microservices made me to forget about that decision, location data is fetched directly from the database.
The complexity of refactoring: average.

3) "Locations" entity-> as was mentioned in Instructions, design should be able to handle a large volume of data, by the way, initially it also had an endpoint:
http://localhost:30001/api/locations
http://localhost:30001/api/locations/<location_id>
-, the first of which accepted Post method for creating new location, and the second one was for retrieving the specified location data (only one location by its id, by initial design), but I decided to omit that endpoints due to the fact it wasn't used in frontend app and my decision not to use that as the endpoint for "Connections" entity microservice, as I did with "persons-api". My design is to separate that service to 3 microservices: 
a) "locations-generator": these containers will get raw data from various sensors, normalize that data and transfer to the second microservice type (see next);
b) "locations2kafka": huge of burstly risen amount of data could make your database software suffer from the load, and where it is the Kafka event streaming platform comes to help us - "locations2kafka" containers put the messages into the Kafka "locations" topic (and it's assumed the overall storage and performance of Kafka cluster will suffice, as it has high horizontal extensibility, which our fictional project laboratory can afford).
And, by the way, the communication channel between "location-generator" and "locations2kafka" was chosen to be the gRPC protocol, it has multiple times more performance gain in comparison with traditional RESTapi communication, especially here it would be so useful, where the performance gain is needed.
—Å) "locations-kafka2db": meanwhile the abovementioned "dirty" and "messy" work is being performed, this type of service containers will fetch with monotonous performance the location data messages from the Kafka topic, parse them and put in DB's "location" table. But this microservice was especially hard to implement for me, as I wanted to escape big code refactoring and was eager to reuse the existing code at first, at second I'm a young junior in coding ;-) and that OOP was too complex to digest and debug. All I wanted to refactor was to insert my py-module source with infinite loop as the main app, which in its turn gets the message from Kafka topic and after parsing calls the 'LocationService.create()' method with the location data dictionary as an argument and that would have been the great and yet simple approach, but all the initial SQLAlchemy code was sticked to Flask's app context, final methods: db.session.add(new_location) with db.session.commit() rose the exception that I can't do that without Flask's 'app' context being initialized. In order to reuse the initial code with a little refactoring I should have used the initial Flask code, but Flask is request/response framework, the only simple method to start the wheel of my infinite loop of Kafka consumer was only to make it as a part of some Flask's route, and call that route with 'curl' or somehow with an http request, for the purpose it would have handled by Flask and start my Kafka Consumer loop within that route. This approach seems ugly for me and I decided to make changes to the code what pushes the data to DB.